{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "871030aa-bc45-4eef-a455-a90edb70a5d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Detecting Severity of Damaged Vehicle Using Accident Images\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/fsi/smart-claims/fsi-claims-flow-4.png?raw=true\" style=\"float: right\" width=\"1000px\">\n",
    "\n",
    "At this point, our data ingestion pipeline has brought in the relevant datasets including policy, claim, and telematics data. \n",
    "\n",
    "To improve our claim processing, we want to be able to automatically detect the incident severity and use this information in our process rules.\n",
    "\n",
    "### Leveraging Databricks Deep Learning AI capabilities on unstructured images\n",
    "\n",
    "When our customers fill claims, they can add images of the incident using their smartphone. These images are captured and saved as part of the claims.\n",
    "\n",
    "In this notebook, we will be using a training dataset containing images with the corresponding incident severity as label (good condition, minor damages or major damages). \n",
    "\n",
    "We'll leverage the Data Intelligence Platform capabilities to fine-tune a state of the art model (`ResNet-50`) to classify our claims images. \n",
    "\n",
    "The model is an AI asset that will be saved in a catalog under Unity Catalog for <b> centralized governance and control.</b>\n",
    "\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=lakehouse&org_id=984752964297111&notebook=%2F02-Data-Science-ML%2F02.1-Model-Training&demo_name=lakehouse-fsi-smart-claims&event=VIEW&path=%2F_dbdemos%2Flakehouse%2Flakehouse-fsi-smart-claims%2F02-Data-Science-ML%2F02.1-Model-Training&version=1&user_hash=3d9550665f4179aba1791a587dee6d56e218186d3a057ad2d2c3ad58a351ed5c\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6ce5aea-eddd-465a-b692-506a82a2f9e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": null
    }
   },
   "source": [
    "### A cluster has been created for this demo\n",
    "To run this demo, just select the cluster `dbdemos-lakehouse-fsi-smart-claims-thomas_hass` from the dropdown menu ([open cluster configuration](https://adb-984752964297111.11.azuredatabricks.net/#setting/clusters/0802-065525-9zzx4cdb/configuration)). <br />\n",
    "*Note: If the cluster was deleted after 30 days, you can re-create it with `dbdemos.create_cluster('lakehouse-fsi-smart-claims')` or re-install the demo: `dbdemos.install('lakehouse-fsi-smart-claims')`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "678f96a6-f7df-47a9-b660-a4e3940d9851",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-sdk==0.39.0 datasets==2.20.0 transformers==4.49.0 tf-keras==2.17.0 accelerate==1.4.0 mlflow==2.20.2 torchvision==0.20.1 deepspeed==0.14.4\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acc7eafc-c0b7-40c4-9448-98f30aeee26c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbfbb94f-70e5-4e7e-a938-5a933fb91aa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Loading and preparing our dataset from Unity Catalog Volume\n",
    "\n",
    "Let's start by reviewing our training dataset. The raw images have been stored and secured using Databricks Volumes, under your Unity Catalog `catalog`.`schema`.\n",
    "\n",
    "Let's ingest our raw images and save them as a Delta table within Unity Catalog. We'll apply some transformation to prepare our training dataset. \n",
    "\n",
    "As our Deep learning model ResNet-50 was trained on 224x224 images, we will resize each picture, adding black background if required. This process will parallelize using spark User Defined Functions (UDF), persisting the binary data back into a Delta Lake table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0126e52-aaf7-4a4d-9abe-b55f52129127",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "training_df = spark.read.format('binaryFile').load(f\"/Volumes/{catalog}/{db}/{volume_name}/Images\")\n",
    "#Extract label from image name\n",
    "training_df = training_df.withColumn(\"label\", regexp_extract(\"path\", r\"/(\\d+)-([a-zA-Z]+)\\.png$\", 2))\n",
    "display(training_df.limit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0b5437c-0160-4906-a973-78fce66a4d3d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Image resizing and preparation"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "from pyspark.sql.functions import pandas_udf, col\n",
    "IMAGE_RESIZE = 224\n",
    "\n",
    "#Resize UDF function\n",
    "@pandas_udf(\"binary\")\n",
    "def resize_image_udf(content_series):\n",
    "  def resize_image(content):\n",
    "    from PIL import Image\n",
    "    \"\"\"resize image and serialize back as jpeg\"\"\"\n",
    "    #Load the PIL image\n",
    "    image = Image.open(io.BytesIO(content))\n",
    "    width, height = image.size   # Get dimensions\n",
    "    new_size = min(width, height)\n",
    "    # Crop the center of the image\n",
    "    image = image.crop(((width - new_size)/2, (height - new_size)/2, (width + new_size)/2, (height + new_size)/2))\n",
    "    #Resize to the new resolution\n",
    "    image = image.resize((IMAGE_RESIZE, IMAGE_RESIZE), Image.NEAREST)\n",
    "    #Save back as jpeg\n",
    "    output = io.BytesIO()\n",
    "    image.save(output, format='JPEG')\n",
    "    return output.getvalue()\n",
    "  return content_series.apply(resize_image)\n",
    "\n",
    "\n",
    "# add the metadata to enable the image preview\n",
    "image_meta = {\"spark.contentAnnotation\" : '{\"mimeType\": \"image/jpeg\"}'}\n",
    "\n",
    "(training_df\n",
    "      .withColumn(\"content\", resize_image_udf(col(\"content\")).alias(\"content\", metadata=image_meta))\n",
    "      .write.mode('overwrite').saveAsTable(\"training_dataset\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a92125b2-83b5-47a3-b75f-18a8141bbf75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(\"training_dataset\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3c148ea-8ba3-47cb-87a7-8fb3b7cca661",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Fine tuning our model with Hugging Face transformers (transfer learning)\n",
    "\n",
    "Our dataset is now ready, and properly labeled.\n",
    "\n",
    "Let's load our Delta Table containing the images and prepare them for hugging face. \n",
    "\n",
    "Databricks makes it easy to convert a Table to a transformers Dataset using the `dataset` libraries.\n",
    "\n",
    "We'll then retrain the base model from its latest checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2b4b52e-2405-40a9-9115-f8207575bf3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "#Setup the training experiment\n",
    "DBDemos.init_experiment_for_batch(\"lakehouse-fsi-smart-claims\", \"hf\")\n",
    "\n",
    "#Note: from_spark support coming with serverless compute - we'll use from_pandas for this simple demo having a small dataset\n",
    "#dataset = Dataset.from_spark(spark.table(\"training_dataset\"), cache_dir=\"/tmp/hf_cache/train\").rename_column(\"content\", \"image\")\n",
    "dataset = Dataset.from_pandas(spark.table(\"training_dataset\").toPandas()).rename_column(\"content\", \"image\")\n",
    "\n",
    "splits = dataset.train_test_split(test_size=0.2, seed = 42)\n",
    "train_ds = splits['train']\n",
    "val_ds = splits['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "022ca6ac-5a5e-4d38-a8f4-d1bbeb42dfe6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Prepare the dataset for our model"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoFeatureExtractor, AutoImageProcessor\n",
    "\n",
    "# pre-trained model from which to fine-tune\n",
    "# Check the hugging face repo for more details & models: https://huggingface.co/microsoft/resnet-50\n",
    "model_checkpoint = \"microsoft/resnet-50\"\n",
    "\n",
    "from PIL import Image\n",
    "import io\n",
    "from torchvision.transforms import CenterCrop, Compose, Normalize, RandomResizedCrop, Resize, ToTensor, Lambda\n",
    "\n",
    "#Extract the model feature (contains info on pre-process step required to transform our data, such as resizing & normalization)\n",
    "#Using the model parameters makes it easy to switch to another model without any change, even if the input size is different.\n",
    "model_def = AutoFeatureExtractor.from_pretrained(model_checkpoint)\n",
    "\n",
    "#Transformations on our training dataset. we'll add some crop here\n",
    "transforms = Compose([Lambda(lambda b: Image.open(io.BytesIO(b)).convert(\"RGB\")), #byte to pil\n",
    "                        ToTensor(), #convert the PIL img to a tensor\n",
    "                        Normalize(mean=model_def.image_mean, std=model_def.image_std)\n",
    "                        ])\n",
    "\n",
    "# Add some random resiz & transformation to our training dataset\n",
    "def preprocess(batch):\n",
    "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
    "    batch[\"image\"] = [transforms(image) for image in batch[\"image\"]]\n",
    "    return batch\n",
    "   \n",
    "#Set our training / validation transformations\n",
    "train_ds.set_transform(preprocess)\n",
    "val_ds.set_transform(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fc9fd76-f6c1-460a-8d77-2a3fff92e004",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load the model"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "\n",
    "#Mapping between class label and value (huggingface use it during inference to output the proper label)\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(set(dataset['label'])):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label\n",
    "    \n",
    "#Load the base model from its checkpoint\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_checkpoint, \n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    num_labels=len(label2id),\n",
    "    ignore_mismatched_sizes = True # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d90ee2e3-e125-4e98-bd29-4da118751538",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Fine tuning our model \n",
    "\n",
    "Our dataset and model is ready. We can now start the training step to fine-tune the model.\n",
    "\n",
    "*Note that for production-grade use-case, we would typically to do some [hyperparameter](https://huggingface.co/docs/transformers/hpo_train) tuning here. We'll keep it simple for this first example and run it with fixed settings.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ea77abb-bf95-4b1c-8a11-bafd54bfa697",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "args = TrainingArguments(\n",
    "    f\"/tmp/huggingface/pcb/{model_name}-finetuned\",\n",
    "    no_cuda=True, #Run on CPU for resnet to make it easier\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    num_train_epochs=20,\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8469e530-3df1-44fe-9c0b-cc6cce0671b3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Model wrapper to package our transform steps with the model"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "# This wrapper adds steps before and after the inference to simplify the model usage\n",
    "# Before calling the model: apply the same transform as the training, resizing the image\n",
    "# After callint the model: only keeps the main class with the probability as output\n",
    "class ModelWrapper(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, pipeline):\n",
    "        self.pipeline = pipeline\n",
    "        # instantiate model in evaluation mode\n",
    "        self.pipeline.model.eval()\n",
    "\n",
    "    def predict(self, context, images):\n",
    "        from PIL import Image\n",
    "        with torch.set_grad_enabled(False):\n",
    "            #Convert the byte to PIL images\n",
    "            images = images['content'].apply(lambda b: Image.open(io.BytesIO(b))).to_list()\n",
    "            #the pipeline returns the probability for all the class\n",
    "            predictions = self.pipeline.predict(images)\n",
    "            #Filter & returns only the class with the highest score [{'score': 0.999038815498352, 'label': 'normal'}, ...]\n",
    "            return pd.DataFrame([max(r, key=lambda x: x['score']) for r in predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d41a036-6dfb-4361-ae6c-9f37a0c6baf0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Start our Training and log the model to MLFlow"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, DefaultDataCollator, EarlyStoppingCallback\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "mlflow.autolog(disable=True)\n",
    "with mlflow.start_run(run_name=\"hugging_face\") as run:\n",
    "  mlflow.log_input(mlflow.data.from_huggingface(train_ds, \"training\"))\n",
    "  def collate_fn(examples):\n",
    "    pixel_values = torch.stack([e[\"image\"] for e in examples])\n",
    "    labels = torch.tensor([label2id[e[\"label\"]] for e in examples], dtype=torch.float)\n",
    "    labels = torch.nn.functional.one_hot(labels.long(), 3).float()\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "  trainer = Trainer(model, args, train_dataset=train_ds, eval_dataset=val_ds, tokenizer=model_def, data_collator=collate_fn) \n",
    "\n",
    "  train_results = trainer.train()\n",
    "  #Build our final hugging face pipeline\n",
    "  classifier = pipeline(\"image-classification\", model=trainer.state.best_model_checkpoint, tokenizer = model_def)\n",
    "\n",
    "  #Wrap the model to easily ingest images and output better results\n",
    "  wrapped_model = ModelWrapper(classifier)\n",
    "  test_df = spark.table(\"training_dataset\").select('content').toPandas()\n",
    "  predictions = wrapped_model.predict(None, test_df)\n",
    "  signature = infer_signature(test_df, predictions)\n",
    "    \n",
    "  reqs = mlflow.transformers.get_default_pip_requirements(model)\n",
    "  #log the model to MLFlow\n",
    "  mlflow.pyfunc.log_model(artifact_path=\"model\", python_model=wrapped_model, pip_requirements=reqs, signature=signature)\n",
    "  mlflow.log_metrics(train_results.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb037fd5-7e01-4f18-b491-969bd86fa773",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Let's now save our model to Unity Catalog\n",
    "\n",
    "Our model is ready, we can easily add it to Unity Catalog and ensure other users can access it.\n",
    "\n",
    "Our Data Engineers or MLOps team will be able to load it to run the inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3cb5664-1d30-4d66-b2c8-60c5cb8be9e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "model_name = \"dbdemos_claims_damage_level\"\n",
    "\n",
    "#Use Databricks Unity Catalog to save our model\n",
    "mlflow.set_registry_uri('databricks-uc')\n",
    "client = MlflowClient()\n",
    "#Add model within our catalog\n",
    "latest_model = mlflow.register_model(f'runs:/{run.info.run_id}/model', f\"{catalog}.{db}.{model_name}\")\n",
    "# Flag it as Production ready using UC Aliases\n",
    "client.set_registered_model_alias(name=f\"{catalog}.{db}.{model_name}\", alias=\"prod\", version=latest_model.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31597a17-37eb-48e0-8ed1-323d92c3024b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Test by loading the model for inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a1b1e00-1e98-42a4-b2f8-d0084ccf4801",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Load back the model\n",
    "predict_damage_udf = mlflow.pyfunc.spark_udf(spark, model_uri=f\"models:/{catalog}.{db}.{model_name}@prod\")\n",
    "columns = predict_damage_udf.metadata.get_input_schema().input_names()\n",
    "#Run the inferences\n",
    "spark.table('training_dataset').withColumn(\"damage_prediction\", predict_damage_udf(*columns)).write.mode('overwrite').saveAsTable('damage_predictions')\n",
    "predictions = spark.table('damage_predictions')\n",
    "display(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff2ac88c-acd5-4a3d-a363-baaf1696077b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model validation\n",
    "We can validate our model using simple visualizations that we store on MLFlow against our actual model binary, input parameters and metrics. This context will be useful when reviewing / auditing our approach.\n",
    "\n",
    "Given the small amount of data our training dataset has, the model is good enough to ship it and start running inferences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d64ec1b-67d3-4d5a-be19-854aa4fb2506",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results = predictions.selectExpr(\"path\", \"label\", \"damage_prediction.label as predictions\", \"damage_prediction.score as score\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d12cbe46-7bd1-4076-9f12-544835681f4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# create confusion matrix\n",
    "confusion_matrix = pd.crosstab(results['label'], results['predictions'])\n",
    "\n",
    "# plot confusion matrix\n",
    "fig = plt.figure()\n",
    "sns.heatmap(confusion_matrix, annot=True, cmap=\"Blues\", fmt='d')\n",
    "client.log_figure(run.info.run_id, fig, \"confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a9bb7f0-7c22-4646-aa94-ed544893f68b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "We demonstrated how Databricks handle unstructured text and let you fine tune Deep Learning model, leveraging <b> Delta and MLFlow </b> to make process easy and reproducible. \n",
    "\n",
    "Our fine-tunbed model is now saved within Unity Catalog. Subsequent access and changes to it can be audited and managed centrally. \n",
    "\n",
    "Now that the model is available, let's see how we can use it for inferencing on new image data.\n",
    "\n",
    "Open the [02.2-Batch-Scoring]($./02.2-Batch-Scoring) notebook to compute the incident severity on all our claims."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02.1-Model-Training",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
